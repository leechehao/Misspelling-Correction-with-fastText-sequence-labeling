Epoch: [1][1/11751] Loss: 8.8899(8.8899) Grad: 6.7993 LR: 0.00010000
Epoch: [1][2500/11751] Loss: 0.1163(0.7807) Grad: 0.4150 LR: 0.00009929
Epoch: [1][5000/11751] Loss: 0.1382(0.4581) Grad: 0.9946 LR: 0.00009858
Epoch: [1][7500/11751] Loss: 0.1182(0.3358) Grad: 1.0437 LR: 0.00009787
Epoch: [1][10000/11751] Loss: 0.0660(0.2708) Grad: 0.4720 LR: 0.00009716
Epoch: [1][11751/11751] Loss: 0.0056(0.2400) Grad: 0.2196 LR: 0.00009667
Training duration: 2376.616 sec
Evaluating Loss: 0.0820
Evaluation duration: 44.774 sec
Accuracy: 0.8028
Epoch: [2][1/11751] Loss: 0.0308(0.0308) Grad: 0.6040 LR: 0.00009667
Epoch: [2][2500/11751] Loss: 0.0039(0.0485) Grad: 0.1720 LR: 0.00009596
Epoch: [2][5000/11751] Loss: 0.0290(0.0469) Grad: 0.3861 LR: 0.00009525
Epoch: [2][7500/11751] Loss: 0.0034(0.0451) Grad: 0.0951 LR: 0.00009454
Epoch: [2][10000/11751] Loss: 0.0461(0.0437) Grad: 0.7797 LR: 0.00009383
Epoch: [2][11751/11751] Loss: 0.0125(0.0428) Grad: 0.7292 LR: 0.00009333
Training duration: 2405.002 sec
Evaluating Loss: 0.0695
Evaluation duration: 43.221 sec
Accuracy: 0.8379
Epoch: [3][1/11751] Loss: 0.0235(0.0235) Grad: 0.3684 LR: 0.00009333
Epoch: [3][2500/11751] Loss: 0.0386(0.0286) Grad: 0.6033 LR: 0.00009262
Epoch: [3][5000/11751] Loss: 0.0641(0.0289) Grad: 0.6267 LR: 0.00009192
Epoch: [3][7500/11751] Loss: 0.0259(0.0290) Grad: 0.5465 LR: 0.00009121
Epoch: [3][10000/11751] Loss: 0.0644(0.0288) Grad: 0.5491 LR: 0.00009050
Epoch: [3][11751/11751] Loss: 0.0005(0.0286) Grad: 0.0228 LR: 0.00009000
Training duration: 2408.161 sec
Evaluating Loss: 0.0635
Evaluation duration: 43.965 sec
Accuracy: 0.8484
Epoch: [4][1/11751] Loss: 0.0144(0.0144) Grad: 0.2922 LR: 0.00009000
Epoch: [4][2500/11751] Loss: 0.0204(0.0211) Grad: 0.4381 LR: 0.00008929
Epoch: [4][5000/11751] Loss: 0.0941(0.0217) Grad: 1.0232 LR: 0.00008858
Epoch: [4][7500/11751] Loss: 0.0012(0.0219) Grad: 0.0396 LR: 0.00008787
Epoch: [4][10000/11751] Loss: 0.0109(0.0221) Grad: 0.4982 LR: 0.00008716
Epoch: [4][11751/11751] Loss: 0.0217(0.0222) Grad: 0.5080 LR: 0.00008667
Training duration: 2410.198 sec
Evaluating Loss: 0.0671
Evaluation duration: 43.407 sec
Accuracy: 0.8438
Epoch: [5][1/11751] Loss: 0.0160(0.0160) Grad: 0.2123 LR: 0.00008667
Epoch: [5][2500/11751] Loss: 0.0265(0.0176) Grad: 0.5538 LR: 0.00008596
Epoch: [5][5000/11751] Loss: 0.0401(0.0179) Grad: 0.2953 LR: 0.00008525
Epoch: [5][7500/11751] Loss: 0.0136(0.0181) Grad: 0.4182 LR: 0.00008454
Epoch: [5][10000/11751] Loss: 0.0133(0.0183) Grad: 0.6000 LR: 0.00008383
Epoch: [5][11751/11751] Loss: 0.0202(0.0183) Grad: 0.5615 LR: 0.00008333
Training duration: 2408.727 sec
Evaluating Loss: 0.0635
Evaluation duration: 43.566 sec
Accuracy: 0.8612
Epoch: [6][1/11751] Loss: 0.0010(0.0010) Grad: 0.0291 LR: 0.00008333
Epoch: [6][2500/11751] Loss: 0.0039(0.0143) Grad: 0.1498 LR: 0.00008262
Epoch: [6][5000/11751] Loss: 0.0110(0.0147) Grad: 0.5148 LR: 0.00008192
Epoch: [6][7500/11751] Loss: 0.0505(0.0149) Grad: 0.5278 LR: 0.00008121
Epoch: [6][10000/11751] Loss: 0.0037(0.0152) Grad: 0.1861 LR: 0.00008050
Epoch: [6][11751/11751] Loss: 0.0042(0.0154) Grad: 0.4923 LR: 0.00008000
Training duration: 2407.986 sec
Evaluating Loss: 0.0634
Evaluation duration: 43.543 sec
Accuracy: 0.8586
Epoch: [7][1/11751] Loss: 0.0016(0.0016) Grad: 0.1271 LR: 0.00008000
Epoch: [7][2500/11751] Loss: 0.0368(0.0124) Grad: 0.6501 LR: 0.00007929
Epoch: [7][5000/11751] Loss: 0.0429(0.0128) Grad: 1.1621 LR: 0.00007858
Epoch: [7][7500/11751] Loss: 0.0048(0.0133) Grad: 0.3237 LR: 0.00007787
Epoch: [7][10000/11751] Loss: 0.0303(0.0133) Grad: 0.6403 LR: 0.00007716
Epoch: [7][11751/11751] Loss: 0.0042(0.0135) Grad: 0.4886 LR: 0.00007667
Training duration: 2406.860 sec
Evaluating Loss: 0.0654
Evaluation duration: 42.956 sec
Accuracy: 0.8580
Epoch: [8][1/11751] Loss: 0.0082(0.0082) Grad: 0.4379 LR: 0.00007667
Epoch: [8][2500/11751] Loss: 0.0912(0.0107) Grad: 0.5480 LR: 0.00007596
Epoch: [8][5000/11751] Loss: 0.0211(0.0110) Grad: 0.8628 LR: 0.00007525
Epoch: [8][7500/11751] Loss: 0.0144(0.0114) Grad: 0.4309 LR: 0.00007454
Epoch: [8][10000/11751] Loss: 0.0034(0.0116) Grad: 0.1708 LR: 0.00007383
Epoch: [8][11751/11751] Loss: 0.0003(0.0117) Grad: 0.0130 LR: 0.00007333
Training duration: 2403.449 sec
Evaluating Loss: 0.0650
Evaluation duration: 42.528 sec
Accuracy: 0.8651
Epoch: [9][1/11751] Loss: 0.0030(0.0030) Grad: 0.3449 LR: 0.00007333
Epoch: [9][2500/11751] Loss: 0.0044(0.0088) Grad: 0.2362 LR: 0.00007262
Epoch: [9][5000/11751] Loss: 0.0253(0.0096) Grad: 1.6784 LR: 0.00007192
Epoch: [9][7500/11751] Loss: 0.0017(0.0099) Grad: 0.1375 LR: 0.00007121
Epoch: [9][10000/11751] Loss: 0.0003(0.0102) Grad: 0.0160 LR: 0.00007050
Epoch: [9][11751/11751] Loss: 0.0004(0.0102) Grad: 0.0260 LR: 0.00007000
Training duration: 2407.489 sec
Evaluating Loss: 0.0654
Evaluation duration: 43.591 sec
Accuracy: 0.8642
Epoch: [10][1/11751] Loss: 0.0010(0.0010) Grad: 0.0631 LR: 0.00007000
Epoch: [10][2500/11751] Loss: 0.0039(0.0086) Grad: 0.1531 LR: 0.00006929
Epoch: [10][5000/11751] Loss: 0.0127(0.0087) Grad: 0.3651 LR: 0.00006858
Epoch: [10][7500/11751] Loss: 0.0002(0.0089) Grad: 0.0060 LR: 0.00006787
Epoch: [10][10000/11751] Loss: 0.0096(0.0090) Grad: 0.5439 LR: 0.00006716
Epoch: [10][11751/11751] Loss: 0.0014(0.0091) Grad: 0.0685 LR: 0.00006667
Training duration: 2351.490 sec
Evaluating Loss: 0.0666
Evaluation duration: 43.298 sec
Accuracy: 0.8680
Epoch: [11][1/11751] Loss: 0.0002(0.0002) Grad: 0.0059 LR: 0.00006667
Epoch: [11][2500/11751] Loss: 0.0031(0.0074) Grad: 0.1826 LR: 0.00006596
Epoch: [11][5000/11751] Loss: 0.0017(0.0076) Grad: 0.1163 LR: 0.00006525
Epoch: [11][7500/11751] Loss: 0.0004(0.0076) Grad: 0.0256 LR: 0.00006454
Epoch: [11][10000/11751] Loss: 0.0202(0.0077) Grad: 0.8212 LR: 0.00006383
Epoch: [11][11751/11751] Loss: 0.0078(0.0079) Grad: 0.7454 LR: 0.00006333
Training duration: 2410.606 sec
Evaluating Loss: 0.0666
Evaluation duration: 42.849 sec
Accuracy: 0.8660
Epoch: [12][1/11751] Loss: 0.0014(0.0014) Grad: 0.1082 LR: 0.00006333
Epoch: [12][2500/11751] Loss: 0.0011(0.0061) Grad: 0.0505 LR: 0.00006262
Epoch: [12][5000/11751] Loss: 0.0038(0.0066) Grad: 0.2422 LR: 0.00006192
Epoch: [12][7500/11751] Loss: 0.0002(0.0068) Grad: 0.0113 LR: 0.00006121
Epoch: [12][10000/11751] Loss: 0.0049(0.0069) Grad: 0.4216 LR: 0.00006050
Epoch: [12][11751/11751] Loss: 0.0147(0.0070) Grad: 1.3387 LR: 0.00006000
Training duration: 2404.847 sec
Evaluating Loss: 0.0732
Evaluation duration: 42.389 sec
Accuracy: 0.8446
Epoch: [13][1/11751] Loss: 0.0015(0.0015) Grad: 0.0846 LR: 0.00006000
Epoch: [13][2500/11751] Loss: 0.0018(0.0052) Grad: 0.1270 LR: 0.00005929
Epoch: [13][5000/11751] Loss: 0.0004(0.0055) Grad: 0.0197 LR: 0.00005858
Epoch: [13][7500/11751] Loss: 0.0174(0.0057) Grad: 0.7731 LR: 0.00005787
Epoch: [13][10000/11751] Loss: 0.0067(0.0058) Grad: 0.3402 LR: 0.00005716
Epoch: [13][11751/11751] Loss: 0.0006(0.0059) Grad: 0.0612 LR: 0.00005667
Training duration: 2409.277 sec
Evaluating Loss: 0.0672
Evaluation duration: 42.764 sec
Accuracy: 0.8755
Epoch: [14][1/11751] Loss: 0.0014(0.0014) Grad: 0.1196 LR: 0.00005667
Epoch: [14][2500/11751] Loss: 0.0006(0.0046) Grad: 0.0351 LR: 0.00005596
Epoch: [14][5000/11751] Loss: 0.0002(0.0049) Grad: 0.0084 LR: 0.00005525
Epoch: [14][7500/11751] Loss: 0.0003(0.0051) Grad: 0.0218 LR: 0.00005454
Epoch: [14][10000/11751] Loss: 0.0012(0.0052) Grad: 0.0752 LR: 0.00005383
Epoch: [14][11751/11751] Loss: 0.0006(0.0053) Grad: 0.0466 LR: 0.00005333
Training duration: 2409.643 sec
Evaluating Loss: 0.0678
Evaluation duration: 42.010 sec
Accuracy: 0.8752
Epoch: [15][1/11751] Loss: 0.0005(0.0005) Grad: 0.0467 LR: 0.00005333
Epoch: [15][2500/11751] Loss: 0.0002(0.0037) Grad: 0.0134 LR: 0.00005262
Epoch: [15][5000/11751] Loss: 0.0056(0.0042) Grad: 0.6685 LR: 0.00005192
Epoch: [15][7500/11751] Loss: 0.0003(0.0043) Grad: 0.0137 LR: 0.00005121
Epoch: [15][10000/11751] Loss: 0.0005(0.0045) Grad: 0.0523 LR: 0.00005050
Epoch: [15][11751/11751] Loss: 0.0047(0.0046) Grad: 0.5484 LR: 0.00005000
Training duration: 2404.038 sec
Evaluating Loss: 0.0672
Evaluation duration: 41.983 sec
Accuracy: 0.8752
Epoch: [16][1/11751] Loss: 0.0006(0.0006) Grad: 0.0226 LR: 0.00005000
Epoch: [16][2500/11751] Loss: 0.0043(0.0033) Grad: 0.5259 LR: 0.00004929
Epoch: [16][5000/11751] Loss: 0.0004(0.0037) Grad: 0.0295 LR: 0.00004858
Epoch: [16][7500/11751] Loss: 0.0001(0.0038) Grad: 0.0069 LR: 0.00004787
Epoch: [16][10000/11751] Loss: 0.0020(0.0038) Grad: 0.1421 LR: 0.00004716
Epoch: [16][11751/11751] Loss: 0.0070(0.0039) Grad: 0.5695 LR: 0.00004667
Training duration: 2405.529 sec
Evaluating Loss: 0.0673
Evaluation duration: 42.321 sec
Accuracy: 0.8755
Epoch: [17][1/11751] Loss: 0.0044(0.0044) Grad: 0.3212 LR: 0.00004667
Epoch: [17][2500/11751] Loss: 0.0164(0.0033) Grad: 0.3797 LR: 0.00004596
Epoch: [17][5000/11751] Loss: 0.0004(0.0032) Grad: 0.0233 LR: 0.00004525
Epoch: [17][7500/11751] Loss: 0.0004(0.0032) Grad: 0.0256 LR: 0.00004454
Epoch: [17][10000/11751] Loss: 0.0241(0.0033) Grad: 0.3295 LR: 0.00004383
Epoch: [17][11751/11751] Loss: 0.0493(0.0033) Grad: 1.1556 LR: 0.00004333
Training duration: 2407.000 sec
Evaluating Loss: 0.0682
Evaluation duration: 42.417 sec
Accuracy: 0.8791
Epoch: [18][1/11751] Loss: 0.0001(0.0001) Grad: 0.0064 LR: 0.00004333
Epoch: [18][2500/11751] Loss: 0.0001(0.0026) Grad: 0.0018 LR: 0.00004262
Epoch: [18][5000/11751] Loss: 0.0087(0.0026) Grad: 0.6563 LR: 0.00004192
Epoch: [18][7500/11751] Loss: 0.0004(0.0027) Grad: 0.0396 LR: 0.00004121
Epoch: [18][10000/11751] Loss: 0.0002(0.0028) Grad: 0.0422 LR: 0.00004050
Epoch: [18][11751/11751] Loss: 0.0089(0.0028) Grad: 1.2274 LR: 0.00004000
Training duration: 2408.856 sec
Evaluating Loss: 0.0689
Evaluation duration: 42.445 sec
Accuracy: 0.8831
Epoch: [19][1/11751] Loss: 0.0002(0.0002) Grad: 0.0163 LR: 0.00004000
Epoch: [19][2500/11751] Loss: 0.0002(0.0020) Grad: 0.0189 LR: 0.00003929
Epoch: [19][5000/11751] Loss: 0.0001(0.0022) Grad: 0.0060 LR: 0.00003858
Epoch: [19][7500/11751] Loss: 0.0002(0.0023) Grad: 0.0120 LR: 0.00003787
Epoch: [19][10000/11751] Loss: 0.0352(0.0024) Grad: 0.8925 LR: 0.00003716
Epoch: [19][11751/11751] Loss: 0.0002(0.0024) Grad: 0.0222 LR: 0.00003667
Training duration: 2407.007 sec
Evaluating Loss: 0.0696
Evaluation duration: 42.827 sec
Accuracy: 0.8823
Epoch: [20][1/11751] Loss: 0.0001(0.0001) Grad: 0.0029 LR: 0.00003667
Epoch: [20][2500/11751] Loss: 0.0005(0.0016) Grad: 0.0323 LR: 0.00003596
Epoch: [20][5000/11751] Loss: 0.0003(0.0018) Grad: 0.0196 LR: 0.00003525
Epoch: [20][7500/11751] Loss: 0.0003(0.0018) Grad: 0.0328 LR: 0.00003454
Epoch: [20][10000/11751] Loss: 0.0001(0.0019) Grad: 0.0078 LR: 0.00003383
Epoch: [20][11751/11751] Loss: 0.0000(0.0019) Grad: 0.0025 LR: 0.00003333
Training duration: 2406.070 sec
Evaluating Loss: 0.0690
Evaluation duration: 42.644 sec
Accuracy: 0.8847
Epoch: [21][1/11751] Loss: 0.0001(0.0001) Grad: 0.0084 LR: 0.00003333
Epoch: [21][2500/11751] Loss: 0.0005(0.0015) Grad: 0.0392 LR: 0.00003262
Epoch: [21][5000/11751] Loss: 0.0006(0.0016) Grad: 0.0441 LR: 0.00003192
Epoch: [21][7500/11751] Loss: 0.0015(0.0016) Grad: 0.2400 LR: 0.00003121
Epoch: [21][10000/11751] Loss: 0.0007(0.0016) Grad: 0.0665 LR: 0.00003050
Epoch: [21][11751/11751] Loss: 0.0000(0.0016) Grad: 0.0005 LR: 0.00003000
Training duration: 2409.100 sec
Evaluating Loss: 0.0700
Evaluation duration: 42.678 sec
Accuracy: 0.8825
Epoch: [22][1/11751] Loss: 0.0121(0.0121) Grad: 0.6930 LR: 0.00003000
Epoch: [22][2500/11751] Loss: 0.0001(0.0013) Grad: 0.0057 LR: 0.00002929
Epoch: [22][5000/11751] Loss: 0.0003(0.0013) Grad: 0.0502 LR: 0.00002858
Epoch: [22][7500/11751] Loss: 0.0000(0.0013) Grad: 0.0014 LR: 0.00002787
Epoch: [22][10000/11751] Loss: 0.0005(0.0013) Grad: 0.0965 LR: 0.00002716
Epoch: [22][11751/11751] Loss: 0.0124(0.0013) Grad: 2.3883 LR: 0.00002667
Training duration: 2351.331 sec
Evaluating Loss: 0.0705
Evaluation duration: 42.843 sec
Accuracy: 0.8847
Epoch: [23][1/11751] Loss: 0.0000(0.0000) Grad: 0.0005 LR: 0.00002667
Epoch: [23][2500/11751] Loss: 0.0001(0.0010) Grad: 0.0118 LR: 0.00002596
Epoch: [23][5000/11751] Loss: 0.0000(0.0010) Grad: 0.0030 LR: 0.00002525
Epoch: [23][7500/11751] Loss: 0.0000(0.0010) Grad: 0.0004 LR: 0.00002454
Epoch: [23][10000/11751] Loss: 0.0000(0.0011) Grad: 0.0011 LR: 0.00002383
Epoch: [23][11751/11751] Loss: 0.0002(0.0011) Grad: 0.0203 LR: 0.00002333
Training duration: 2351.243 sec
Evaluating Loss: 0.0708
Evaluation duration: 42.764 sec
Accuracy: 0.8901
Epoch: [24][1/11751] Loss: 0.0000(0.0000) Grad: 0.0004 LR: 0.00002333
Epoch: [24][2500/11751] Loss: 0.0001(0.0007) Grad: 0.0032 LR: 0.00002262
Epoch: [24][5000/11751] Loss: 0.0004(0.0008) Grad: 0.0397 LR: 0.00002192
Epoch: [24][7500/11751] Loss: 0.0001(0.0008) Grad: 0.0072 LR: 0.00002121
Epoch: [24][10000/11751] Loss: 0.0000(0.0008) Grad: 0.0006 LR: 0.00002050
Epoch: [24][11751/11751] Loss: 0.0000(0.0008) Grad: 0.0002 LR: 0.00002000
Training duration: 2415.951 sec
Evaluating Loss: 0.0702
Evaluation duration: 42.567 sec
Accuracy: 0.8887
Epoch: [25][1/11751] Loss: 0.0000(0.0000) Grad: 0.0026 LR: 0.00002000
Epoch: [25][2500/11751] Loss: 0.0000(0.0006) Grad: 0.0016 LR: 0.00001929
Epoch: [25][5000/11751] Loss: 0.0000(0.0006) Grad: 0.0018 LR: 0.00001858
Epoch: [25][7500/11751] Loss: 0.0000(0.0007) Grad: 0.0010 LR: 0.00001787
Epoch: [25][10000/11751] Loss: 0.0000(0.0007) Grad: 0.0045 LR: 0.00001716
Epoch: [25][11751/11751] Loss: 0.0002(0.0006) Grad: 0.0110 LR: 0.00001667
Training duration: 2411.683 sec
Evaluating Loss: 0.0728
Evaluation duration: 43.461 sec
Accuracy: 0.8884
Epoch: [26][1/11751] Loss: 0.0000(0.0000) Grad: 0.0008 LR: 0.00001667
Epoch: [26][2500/11751] Loss: 0.0000(0.0005) Grad: 0.0008 LR: 0.00001596
Epoch: [26][5000/11751] Loss: 0.0000(0.0005) Grad: 0.0008 LR: 0.00001525
Epoch: [26][7500/11751] Loss: 0.0003(0.0005) Grad: 0.0519 LR: 0.00001454
Epoch: [26][10000/11751] Loss: 0.0000(0.0004) Grad: 0.0026 LR: 0.00001383
Epoch: [26][11751/11751] Loss: 0.0000(0.0005) Grad: 0.0035 LR: 0.00001333
Training duration: 2353.255 sec
Evaluating Loss: 0.0731
Evaluation duration: 43.073 sec
Accuracy: 0.8907
Epoch: [27][1/11751] Loss: 0.0000(0.0000) Grad: 0.0006 LR: 0.00001333
Epoch: [27][2500/11751] Loss: 0.0001(0.0003) Grad: 0.0240 LR: 0.00001262
Epoch: [27][5000/11751] Loss: 0.0001(0.0004) Grad: 0.0212 LR: 0.00001192
Epoch: [27][7500/11751] Loss: 0.0000(0.0004) Grad: 0.0021 LR: 0.00001121
Epoch: [27][10000/11751] Loss: 0.0000(0.0004) Grad: 0.0022 LR: 0.00001050
Epoch: [27][11751/11751] Loss: 0.0000(0.0004) Grad: 0.0005 LR: 0.00001000
Training duration: 2412.369 sec
Evaluating Loss: 0.0733
Evaluation duration: 42.930 sec
Accuracy: 0.8891
Epoch: [28][1/11751] Loss: 0.0000(0.0000) Grad: 0.0010 LR: 0.00001000
Epoch: [28][2500/11751] Loss: 0.0001(0.0002) Grad: 0.0062 LR: 0.00000929
Epoch: [28][5000/11751] Loss: 0.0001(0.0002) Grad: 0.0113 LR: 0.00000858
Epoch: [28][7500/11751] Loss: 0.0001(0.0002) Grad: 0.0051 LR: 0.00000787
Epoch: [28][10000/11751] Loss: 0.0068(0.0002) Grad: 0.1269 LR: 0.00000716
Epoch: [28][11751/11751] Loss: 0.0000(0.0002) Grad: 0.0001 LR: 0.00000667
Training duration: 2413.516 sec
Evaluating Loss: 0.0753
Evaluation duration: 42.684 sec
Accuracy: 0.8915
Epoch: [29][1/11751] Loss: 0.0004(0.0004) Grad: 0.0194 LR: 0.00000667
Epoch: [29][2500/11751] Loss: 0.0000(0.0002) Grad: 0.0002 LR: 0.00000596
Epoch: [29][5000/11751] Loss: 0.0019(0.0002) Grad: 0.0900 LR: 0.00000525
Epoch: [29][7500/11751] Loss: 0.0000(0.0002) Grad: 0.0003 LR: 0.00000454
Epoch: [29][10000/11751] Loss: 0.0000(0.0002) Grad: 0.0004 LR: 0.00000383
Epoch: [29][11751/11751] Loss: 0.0000(0.0002) Grad: 0.0000 LR: 0.00000333
Training duration: 2410.268 sec
Evaluating Loss: 0.0762
Evaluation duration: 43.042 sec
Accuracy: 0.8935
Epoch: [30][1/11751] Loss: 0.0000(0.0000) Grad: 0.0005 LR: 0.00000333
Epoch: [30][2500/11751] Loss: 0.0001(0.0001) Grad: 0.0059 LR: 0.00000262
Epoch: [30][5000/11751] Loss: 0.0000(0.0001) Grad: 0.0006 LR: 0.00000192
Epoch: [30][7500/11751] Loss: 0.0000(0.0001) Grad: 0.0009 LR: 0.00000121
Epoch: [30][10000/11751] Loss: 0.0000(0.0001) Grad: 0.0002 LR: 0.00000050
Epoch: [30][11751/11751] Loss: 0.0000(0.0001) Grad: 0.0000 LR: 0.00000000
Training duration: 2411.327 sec
Evaluating Loss: 0.0761
Evaluation duration: 42.812 sec
Accuracy: 0.8930
